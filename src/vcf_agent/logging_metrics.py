"""
logging_metrics.py
Structured logging and Prometheus metrics for AI agent/LLM interactions.
- Uses structlog for JSON logs
- Uses prometheus_client for metrics
- Exposes /metrics endpoint for Prometheus scraping
"""

import structlog
import logging
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time
import threading

# --- Structured Logging Setup ---

def setup_logging():
    logging.basicConfig(level=logging.INFO)
    structlog.configure(
        processors=[
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.JSONRenderer()
        ],
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory()
    )
    return structlog.get_logger()

log = setup_logging()

# --- Prometheus Metrics Setup ---

REQUEST_COUNT = Counter(
    'ai_requests_total', 'Total AI agent requests', ['model', 'status']
)
RESPONSE_TIME = Histogram(
    'ai_response_seconds', 'AI agent response time (seconds)', ['model']
)
TOKEN_COUNT = Counter(
    'ai_tokens_total', 'Total tokens generated by the AI agent', ['model']
)
ERROR_TYPE_COUNT = Counter(
    'ai_errors_total', 'Total errors by type', ['model', 'error_type']
)
CONCURRENT_REQUESTS = Gauge(
    'ai_concurrent_requests', 'Number of concurrent AI agent requests', ['model']
)
# Optionally: concurrent requests, token usage, etc.

# --- Metrics HTTP Endpoint ---

def start_metrics_server(port=8000):
    # Start Prometheus metrics server in a background thread
    thread = threading.Thread(target=start_http_server, args=(port,), daemon=True)
    thread.start()

# --- Example Instrumentation ---
def log_ai_interaction(user, model, prompt, response, latency, tokens, success=True, error=None, error_type=None):
    CONCURRENT_REQUESTS.labels(model=model).inc()
    try:
        log.info(
            "ai_interaction",
            user=user,
            model=model,
            prompt=prompt,
            response=response,
            latency=latency,
            tokens=tokens,
            success=success,
            error=error,
            error_type=error_type
        )
        status = "success" if success else "error"
        REQUEST_COUNT.labels(model=model, status=status).inc()
        RESPONSE_TIME.labels(model=model).observe(latency)
        TOKEN_COUNT.labels(model=model).inc(tokens)
        if not success and error_type:
            ERROR_TYPE_COUNT.labels(model=model, error_type=error_type).inc()
    finally:
        CONCURRENT_REQUESTS.labels(model=model).dec()

# --- Example Usage ---
if __name__ == "__main__":
    start_metrics_server(8000)
    # Simulate an AI interaction
    user = "user123"
    model = "gpt-4"
    prompt = "Summarize this VCF file."
    start = time.time()
    try:
        # Simulate response
        response = "Summary: ..."
        tokens = 42
        latency = time.time() - start
        log_ai_interaction(user, model, prompt, response, latency, tokens, success=True)
    except Exception as e:
        latency = time.time() - start
        log_ai_interaction(user, model, prompt, None, latency, 0, success=False, error=str(e))

# --- Metrics Documentation ---
# ai_requests_total: Counter of total requests by model and status (success/error)
# ai_response_seconds: Histogram of response times by model
# ai_tokens_total: Counter of total tokens generated by model
# ai_errors_total: Counter of errors by model and error type
# ai_concurrent_requests: Gauge of concurrent requests by model 